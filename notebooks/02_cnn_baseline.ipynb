{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Classification: CNN Baseline Model\n",
        "\n",
        "**Course:** CSCI 6366 (Neural Networks and Deep Learning)  \n",
        "**Project:** Audio Classification using CNN  \n",
        "**Notebook:** Baseline CNN Model Implementation\n",
        "\n",
        "## 0. What this notebook will do (big picture)\n",
        "\n",
        "By the end of this new notebook, we want to have:\n",
        "\n",
        "1. A **fixed-size input representation** for each audio clip:\n",
        "   â†’ Mel-spectrogram shaped like `(128, 128, 1)` (height Ã— width Ã— channels).\n",
        "\n",
        "2. A **small CNN model** defined in Keras:\n",
        "\n",
        "   ```python\n",
        "   Conv2D â†’ ReLU â†’ MaxPooling2D â†’ Conv2D â†’ MaxPooling2D â†’ Flatten â†’ Dense â†’ Softmax\n",
        "   ```\n",
        "\n",
        "3. The model **compiled** with:\n",
        "   * `loss='categorical_crossentropy'`\n",
        "   * `optimizer='adam'`\n",
        "   * `metrics=['accuracy']`\n",
        "\n",
        "We'll focus today on:\n",
        "* shaping the data,\n",
        "* building the model,\n",
        "* understanding every layer.\n",
        "\n",
        "We won't worry about perfect training yet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. First cell: imports\n",
        "\n",
        "### What and why\n",
        "\n",
        "* `numpy` â†’ arrays and math.\n",
        "* `Path` â†’ nice file paths.\n",
        "* `librosa` â†’ we'll reuse it to build Mel-spectrograms.\n",
        "* `tensorflow` / `keras`:\n",
        "  * `layers` â†’ Conv2D, MaxPooling2D, Flatten, Dense, etc.\n",
        "  * `models` â†’ to create the `Sequential` model.\n",
        "\n",
        "If you get an error on TensorFlow, install it in your venv:\n",
        "\n",
        "```bash\n",
        "(.venv) pip install \"tensorflow>=2.16,<3\"\n",
        "```\n",
        "\n",
        "(run in the terminal, not in the notebook).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Where our audio data lives (relative to this notebook in notebooks/)\n",
        "DATA_DIR = Path(\"../data\").resolve()\n",
        "\n",
        "# Our three classes\n",
        "CLASS_NAMES = [\"dog\", \"cat\", \"bird\"]\n",
        "\n",
        "label_to_index = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
        "label_to_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Second cell: basic config and class labels\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* `DATA_DIR` points to your `data/` folder from this notebook (`../data`).\n",
        "* `CLASS_NAMES` is the list of class labels (folder names under `data/`).\n",
        "* `label_to_index` turns labels into numbers:\n",
        "  * `\"dog\" â†’ 0`, `\"cat\" â†’ 1`, `\"bird\" â†’ 2`.\n",
        "* This mapping will be used for training targets.\n",
        "\n",
        "Run the cell and you should see something like:\n",
        "\n",
        "```python\n",
        "{'dog': 0, 'cat': 1, 'bird': 2}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mel_spectrogram(\n",
        "    audio_path: Path,\n",
        "    sr: int = 16000,\n",
        "    n_fft: int = 1024,\n",
        "    hop_length: int = 512,\n",
        "    n_mels: int = 128,\n",
        ") -> tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Load an audio file and compute its Mel-spectrogram in dB scale.\n",
        "\n",
        "    Returns:\n",
        "        S_db: 2D array of shape (n_mels, time_frames), Mel-spectrogram in dB.\n",
        "        sr: sample rate used.\n",
        "    \"\"\"\n",
        "    # 1. Load waveform, resampled to `sr` if needed\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # 2. Compute Mel-spectrogram (power)\n",
        "    S = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels,\n",
        "        power=2.0,\n",
        "    )\n",
        "\n",
        "    # 3. Convert to dB scale\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    return S_db, sr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Third cell: copy in `load_mel_spectrogram` (with slight tweak)\n",
        "\n",
        "We'll reuse the helper we built, but only need it to return `S_db` and `sr` for now.\n",
        "\n",
        "### Quick recap of what it does\n",
        "\n",
        "* **Input**: path to `.wav` file.\n",
        "* **Inside**:\n",
        "  * Load waveform `y` at `sr` (16k).\n",
        "  * Compute Mel-spectrogram `(n_mels Ã— time_frames)`.\n",
        "  * Convert to dB (log scale).\n",
        "* **Output**: `S_db` (2D spectrogram), `sr`.\n",
        "\n",
        "This function is **our bridge** from \"raw audio file\" â†’ \"2D array we'll feed into the CNN\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_or_crop_spectrogram(S_db: np.ndarray, target_shape=(128, 128)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ensure the Mel-spectrogram has shape (target_height, target_width)\n",
        "    by centrally cropping or zero-padding along the time axis.\n",
        "\n",
        "    Assumes S_db shape is (n_mels, time_frames).\n",
        "    \"\"\"\n",
        "    target_height, target_width = target_shape\n",
        "    n_mels, time_frames = S_db.shape\n",
        "\n",
        "    # 1. If mel dimension doesn't match target_height, we could pad/crop,\n",
        "    #    but here we assume n_mels == target_height (128).\n",
        "    if n_mels != target_height:\n",
        "        raise ValueError(f\"Expected {target_height} mel bands, got {n_mels}\")\n",
        "\n",
        "    # 2. If too many time frames: centrally crop to target_width\n",
        "    if time_frames > target_width:\n",
        "        start = (time_frames - target_width) // 2\n",
        "        end = start + target_width\n",
        "        S_db = S_db[:, start:end]\n",
        "\n",
        "    # 3. If too few time frames: pad with zeros on the right\n",
        "    elif time_frames < target_width:\n",
        "        pad_width = target_width - time_frames\n",
        "        S_db = np.pad(\n",
        "            S_db,\n",
        "            pad_width=((0, 0), (0, pad_width)),  # only pad time axis on the right\n",
        "            mode=\"constant\",\n",
        "            constant_values=(S_db.min(),),\n",
        "        )\n",
        "\n",
        "    # Now S_db has shape (target_height, target_width)\n",
        "    return S_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fourth cell: make the spectrogram a fixed size (128Ã—128)\n",
        "\n",
        "Right now different clips might have different `time_frames` (widths), depending on duration.\n",
        "\n",
        "CNNs want **fixed shape** input. So we'll:\n",
        "* Keep height = `n_mels = 128`.\n",
        "* Force width = `128` by:\n",
        "  * **If too long** â†’ cut the center to 128 columns.\n",
        "  * **If too short** â†’ pad with zeros on the right.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* `S_db.shape` gives `(n_mels, time_frames)`.\n",
        "* We **expect** `n_mels = 128`, and want `time_frames = 128`.\n",
        "* If `time_frames > 128`:\n",
        "  * We compute a `start` index so we crop the **center** portion.\n",
        "* If `time_frames < 128`:\n",
        "  * We pad columns on the right with the **minimum** value (darkest color).\n",
        "\n",
        "Result: every clip becomes a **128Ã—128 matrix**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_example_for_model(audio_path: Path, label: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load one audio file and return:\n",
        "      X: Mel-spectrogram as float32 array with shape (128, 128, 1)\n",
        "      y: one-hot encoded label array with shape (num_classes,)\n",
        "    \"\"\"\n",
        "    # 1. Mel-spectrogram in dB\n",
        "    S_db, sr = load_mel_spectrogram(audio_path)\n",
        "\n",
        "    # 2. Ensure fixed size 128x128\n",
        "    S_fixed = pad_or_crop_spectrogram(S_db, target_shape=(128, 128))\n",
        "\n",
        "    # 3. Normalize (optional but common): scale to [0, 1]\n",
        "    #    We shift and scale based on min and max of this spectrogram\n",
        "    S_min = S_fixed.min()\n",
        "    S_max = S_fixed.max()\n",
        "    S_norm = (S_fixed - S_min) / (S_max - S_min + 1e-8)  # avoid divide-by-zero\n",
        "\n",
        "    # 4. Add channel dimension â†’ (128, 128, 1)\n",
        "    X = S_norm.astype(\"float32\")[..., np.newaxis]\n",
        "\n",
        "    # 5. Build one-hot label vector\n",
        "    num_classes = len(CLASS_NAMES)\n",
        "    y = np.zeros(num_classes, dtype=\"float32\")\n",
        "    y[label_to_index[label]] = 1.0\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fifth cell: convert audio file â†’ model-ready input & label\n",
        "\n",
        "Now let's make a function that:\n",
        "* Takes:\n",
        "  * `audio_path`\n",
        "  * `label` (e.g., `\"dog\"`)\n",
        "* Returns:\n",
        "  * `X`: spectrogram with shape `(128, 128, 1)` (extra channel dimension).\n",
        "  * `y`: one-hot label like `[1, 0, 0]` for dog.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Get Mel-spectrogram**:\n",
        "   ```python\n",
        "   S_db, sr = load_mel_spectrogram(audio_path)\n",
        "   ```\n",
        "\n",
        "2. **Make it 128Ã—128**:\n",
        "   ```python\n",
        "   S_fixed = pad_or_crop_spectrogram(S_db, target_shape=(128, 128))\n",
        "   ```\n",
        "\n",
        "3. **Normalize**:\n",
        "   ```python\n",
        "   S_min = S_fixed.min()\n",
        "   S_max = S_fixed.max()\n",
        "   S_norm = (S_fixed - S_min) / (S_max - S_min + 1e-8)\n",
        "   ```\n",
        "   * This maps values to roughly `[0, 1]`.\n",
        "   * Normalization helps training.\n",
        "\n",
        "4. **Add channel dimension**:\n",
        "   ```python\n",
        "   X = S_norm.astype(\"float32\")[..., np.newaxis]\n",
        "   ```\n",
        "   * `S_norm` shape: `(128, 128)`\n",
        "   * `[..., np.newaxis]` â†’ `(128, 128, 1)`\n",
        "     (like a grayscale image with 1 channel).\n",
        "\n",
        "5. **One-hot label**:\n",
        "   ```python\n",
        "   y = np.zeros(num_classes)\n",
        "   y[label_to_index[label]] = 1.0\n",
        "   ```\n",
        "   * For `\"dog\"` (index 0): `[1.0, 0.0, 0.0]`\n",
        "   * For `\"cat\"`: `[0.0, 1.0, 0.0]`, etc.\n",
        "\n",
        "This `(X, y)` pair is exactly what we'll feed to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_dataset(max_files_per_class: int = 20):\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for label in CLASS_NAMES:\n",
        "        class_dir = DATA_DIR / label\n",
        "        wav_files = sorted(class_dir.glob(\"*.wav\"))\n",
        "\n",
        "        for audio_path in wav_files[:max_files_per_class]:\n",
        "            X, y = load_example_for_model(audio_path, label)\n",
        "            X_list.append(X)\n",
        "            y_list.append(y)\n",
        "\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    y = np.stack(y_list, axis=0)\n",
        "    return X, y\n",
        "\n",
        "X, y = load_dataset(max_files_per_class=20)\n",
        "X.shape, y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sixth cell: build a tiny dataset (even just a few examples)\n",
        "\n",
        "We'll keep it simple: load a handful of files from each folder for now.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* Loop over `\"dog\"`, `\"cat\"`, `\"bird\"`.\n",
        "* For each class:\n",
        "  * Find `.wav` files inside that folder.\n",
        "  * Take at most `max_files_per_class`.\n",
        "  * Convert each to `(X, y)` using our helper.\n",
        "* `X_list` is a Python list of arrays with shape `(128,128,1)`.\n",
        "* `np.stack` turns it into a big 4D tensor:\n",
        "  * `X.shape = (N, 128, 128, 1)`\n",
        "    (N = total number of samples).\n",
        "  * `y.shape = (N, 3)` (3 classes).\n",
        "\n",
        "This gives us a small dataset to test our model pipeline.\n",
        "\n",
        "> If this is slow, you can reduce `max_files_per_class` to 5 or 10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_shape = (128, 128, 1)\n",
        "num_classes = len(CLASS_NAMES)\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Block 1\n",
        "    layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "        input_shape=input_shape,\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten + Dense\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Seventh cell: build the CNN model\n",
        "\n",
        "Now the fun part: define the CNN.\n",
        "\n",
        "### Detailed explanation of each part\n",
        "\n",
        "#### `input_shape = (128, 128, 1)`\n",
        "\n",
        "* Height = 128 (mel bands),\n",
        "* Width = 128 (time frames),\n",
        "* Channels = 1 (grayscale spectrogram).\n",
        "\n",
        "#### `Conv2D` (first block)\n",
        "\n",
        "```python\n",
        "layers.Conv2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    activation=\"relu\",\n",
        "    padding=\"same\",\n",
        "    input_shape=input_shape,\n",
        "),\n",
        "```\n",
        "\n",
        "* **filters=32**:\n",
        "  * Learn 32 different 3Ã—3 filters.\n",
        "  * Output will have 32 feature maps.\n",
        "* **kernel_size=(3,3)**:\n",
        "  * Each filter looks at a 3Ã—3 local patch.\n",
        "* **activation=\"relu\"**:\n",
        "  * Apply `ReLU(x) = max(0,x)` after convolution.\n",
        "* **padding=\"same\"**:\n",
        "  * Pad edges so that output height/width stay the same as input.\n",
        "* **input_shape**:\n",
        "  * Only needed in first layer:\n",
        "    * Tells Keras what input shape to expect.\n",
        "\n",
        "So after this layer:\n",
        "* Input: `(128, 128, 1)`\n",
        "* Output: `(128, 128, 32)` (same H,W, but now 32 channels).\n",
        "\n",
        "#### First `MaxPooling2D`\n",
        "\n",
        "```python\n",
        "layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "```\n",
        "\n",
        "* Pool size 2Ã—2:\n",
        "  * Each 2Ã—2 block in each feature map becomes 1 value.\n",
        "  * Downsamples height and width by factor 2.\n",
        "* Output:\n",
        "  * `(64, 64, 32)`.\n",
        "\n",
        "#### Second Conv block\n",
        "\n",
        "```python\n",
        "layers.Conv2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    activation=\"relu\",\n",
        "    padding=\"same\",\n",
        "),\n",
        "layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "```\n",
        "\n",
        "* Now we have 64 filters.\n",
        "* Input to this Conv2D: `(64, 64, 32)`.\n",
        "* Output of Conv2D: `(64, 64, 64)`.\n",
        "* After MaxPooling2D:\n",
        "  * `(32, 32, 64)`.\n",
        "\n",
        "At this stage, we have a small 32Ã—32 spatial map with 64 channels â†’ pretty abstract features.\n",
        "\n",
        "#### Flatten + Dense\n",
        "\n",
        "```python\n",
        "layers.Flatten(),\n",
        "layers.Dense(64, activation=\"relu\"),\n",
        "layers.Dense(num_classes, activation=\"softmax\"),\n",
        "```\n",
        "\n",
        "* **Flatten**:\n",
        "  * Take `(32, 32, 64)` and convert to 1D vector:\n",
        "    * size = `32 * 32 * 64 = 65536`.\n",
        "* **Dense(64, relu)**:\n",
        "  * Fully connected layer with 64 neurons.\n",
        "  * Combines all extracted features into a compact representation.\n",
        "* **Dense(num_classes, softmax)**:\n",
        "  * Last layer with 3 neurons (dog/cat/bird).\n",
        "  * `softmax` makes outputs sum to 1 and interpretable as probabilities.\n",
        "\n",
        "`model.summary()` prints all shapes and parameter counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Eighth cell: compile the model\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* **optimizer=\"adam\"**:\n",
        "  * Adam is a widely used variant of gradient descent.\n",
        "  * It adapts the learning rate per parameter; good default choice.\n",
        "\n",
        "* **loss=\"categorical_crossentropy\"**:\n",
        "  * Suitable when:\n",
        "    * We have **multi-class** classification (dog vs cat vs bird).\n",
        "    * Labels are **one-hot** vectors `[1,0,0]`, `[0,1,0]`, etc.\n",
        "  * Measures how different the predicted probability distribution is from the true one.\n",
        "\n",
        "* **metrics=[\"accuracy\"]**:\n",
        "  * In training logs, show the portion of examples where:\n",
        "    * `argmax(predictions) == argmax(true_labels)`.\n",
        "\n",
        "Now the model is ready to train.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    validation_split=0.2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. (Optional) Ninth cell: sanity-check a tiny training run\n",
        "\n",
        "You *don't* have to run long training now, but we can test that everything is wired correctly.\n",
        "\n",
        "If this runs without shape errors, your entire pipeline:\n",
        "\n",
        "`.wav â†’ Mel-spectrogram â†’ 128Ã—128Ã—1 â†’ CNN â†’ softmax`\n",
        "\n",
        "is working ðŸŽ‰\n",
        "\n",
        "---\n",
        "\n",
        "## What we can do next (after you try this)\n",
        "\n",
        "Once you run through these cells:\n",
        "\n",
        "1. If **any** cell errors, send me:\n",
        "   * The code cell.\n",
        "   * The full error message.\n",
        "     I'll debug and explain what went wrong and why.\n",
        "\n",
        "2. Concept-wise, we can:\n",
        "   * Take the `model.summary()` output and I'll walk through **every line** so you understand shapes & parameters.\n",
        "   * Talk more about:\n",
        "     * What the filters may be learning in the first vs second conv layer.\n",
        "     * How receptive field grows.\n",
        "\n",
        "Then, once this baseline is solid, we can think about:\n",
        "* Splitting properly into train/val/test.\n",
        "* Data augmentation (adding noise, time shifting).\n",
        "* Improving model depth.\n",
        "\n",
        "But first: get this notebook working and understanding every step.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
