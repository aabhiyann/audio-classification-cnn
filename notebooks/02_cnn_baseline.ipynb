{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Classification: CNN Baseline Model\n",
        "\n",
        "**Course:** CSCI 6366 (Neural Networks and Deep Learning)  \n",
        "**Project:** Audio Classification using CNN  \n",
        "**Notebook:** Baseline CNN Model Implementation\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements a baseline Convolutional Neural Network (CNN) for audio classification. We will:\n",
        "1. Load and preprocess audio files into fixed-size mel-spectrogram representations\n",
        "2. Build a simple CNN architecture using Keras/TensorFlow\n",
        "3. Train and evaluate the baseline model on the audio classification task\n",
        "\n",
        "The goal is to establish a baseline model that can classify audio samples into categories (dog, cat, bird).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What this notebook will do (big picture)\n",
        "\n",
        "By the end of this new notebook, we want to have:\n",
        "\n",
        "A fixed-size input representation for each audio clip:\n",
        "→ Mel-spectrogram shaped like (128, 128, 1) (height × width × channels).\n",
        "\n",
        "A small CNN model defined in Keras:\n",
        "\n",
        "Conv2D → ReLU → MaxPooling2D → Conv2D → MaxPooling2D → Flatten → Dense → Softmax\n",
        "\n",
        "\n",
        "The model compiled with:\n",
        "\n",
        "loss='categorical_crossentropy'\n",
        "\n",
        "optimizer='adam'\n",
        "\n",
        "metrics=['accuracy']\n",
        "\n",
        "We’ll focus today on:\n",
        "\n",
        "shaping the data,\n",
        "\n",
        "building the model,\n",
        "\n",
        "understanding every layer.\n",
        "\n",
        "We won’t worry about perfect training yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dog': 0, 'cat': 1, 'bird': 2}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Configuration and Data Paths\n",
        "\n",
        "Set up the data directory path and define the class labels for our audio classification task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Where our audio data lives (relative to this notebook in notebooks/)\n",
        "DATA_DIR = Path(\"../data\").resolve()\n",
        "\n",
        "# Our three classes\n",
        "CLASS_NAMES = [\"dog\", \"cat\", \"bird\"]\n",
        "\n",
        "# Create label-to-index mapping for one-hot encoding\n",
        "label_to_index = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
        "print(\"Label to index mapping:\", label_to_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Configuration Details\n",
        "\n",
        "- **`DATA_DIR`**: Points to the `data/` folder relative to this notebook (`../data`)\n",
        "- **`CLASS_NAMES`**: List of class labels corresponding to folder names under `data/`\n",
        "- **`label_to_index`**: Mapping that converts string labels to numeric indices:\n",
        "  - `\"dog\" → 0`\n",
        "  - `\"cat\" → 1`\n",
        "  - `\"bird\" → 2`\n",
        "\n",
        "This mapping will be used to create one-hot encoded training targets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOw, make the spectrogram a fixed size (128×128)\n",
        "\n",
        "Right now different clips might have different time_frames (widths), depending on duration.\n",
        "\n",
        "CNNs want fixed shape input. So we’ll:\n",
        "\n",
        "Keep height = n_mels = 128.\n",
        "\n",
        "Force width = 128 by:\n",
        "\n",
        "If too long → cut the center to 128 columns.\n",
        "\n",
        "If too short → pad with zeros on the right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mel_spectrogram(\n",
        "    audio_path: Path,\n",
        "    sr: int = 16000,\n",
        "    n_fft: int = 1024,\n",
        "    hop_length: int = 512,\n",
        "    n_mels: int = 128,\n",
        ") -> tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Load an audio file and compute its Mel-spectrogram in dB scale.\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file\n",
        "        sr: Sample rate (default: 16000 Hz)\n",
        "        n_fft: FFT window size (default: 1024)\n",
        "        hop_length: Number of samples between successive frames (default: 512)\n",
        "        n_mels: Number of mel filter banks (default: 128)\n",
        "\n",
        "    Returns:\n",
        "        S_db: 2D array of shape (n_mels, time_frames), Mel-spectrogram in dB\n",
        "        sr: Sample rate used\n",
        "    \"\"\"\n",
        "    # 1. Load waveform, resampled to `sr` if needed\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # 2. Compute Mel-spectrogram (power)\n",
        "    S = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels,\n",
        "        power=2.0,\n",
        "    )\n",
        "\n",
        "    # 3. Convert to dB scale\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    return S_db, sr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def pad_or_crop_spectrogram(S_db: np.ndarray, target_shape=(128, 128)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ensure the Mel-spectrogram has shape (target_height, target_width)\n",
        "    by centrally cropping or zero-padding along the time axis.\n",
        "\n",
        "    Args:\n",
        "        S_db: Mel-spectrogram array with shape (n_mels, time_frames)\n",
        "        target_shape: Target (height, width) tuple\n",
        "\n",
        "    Returns:\n",
        "        Fixed-size spectrogram with shape (target_height, target_width)\n",
        "    \"\"\"\n",
        "    target_height, target_width = target_shape\n",
        "    n_mels, time_frames = S_db.shape\n",
        "\n",
        "    # 1. Validate mel dimension matches target_height\n",
        "    if n_mels != target_height:\n",
        "        raise ValueError(f\"Expected {target_height} mel bands, got {n_mels}\")\n",
        "\n",
        "    # 2. If too many time frames: centrally crop to target_width\n",
        "    if time_frames > target_width:\n",
        "        start = (time_frames - target_width) // 2\n",
        "        end = start + target_width\n",
        "        S_db = S_db[:, start:end]\n",
        "\n",
        "    # 3. If too few time frames: pad with minimum value on the right\n",
        "    elif time_frames < target_width:\n",
        "        pad_width = target_width - time_frames\n",
        "        S_db = np.pad(\n",
        "            S_db,\n",
        "            pad_width=((0, 0), (0, pad_width)),  # only pad time axis on the right\n",
        "            mode=\"constant\",\n",
        "            constant_values=(S_db.min(),),\n",
        "        )\n",
        "\n",
        "    # Now S_db has shape (target_height, target_width)\n",
        "    return S_db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "convert audio file → model-ready input & label\n",
        "\n",
        "Now let’s make a function that:\n",
        "\n",
        "Takes:\n",
        "\n",
        "audio_path\n",
        "\n",
        "label (e.g., \"dog\")\n",
        "\n",
        "Returns:\n",
        "\n",
        "X: spectrogram with shape (128, 128, 1) (extra channel dimension).\n",
        "\n",
        "y: one-hot label like [1, 0, 0] for dog."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_example_for_model(audio_path: Path, label: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load one audio file and convert it to model-ready format.\n",
        "\n",
        "    Args:\n",
        "        audio_path: Path to the audio file\n",
        "        label: String label (e.g., \"dog\", \"cat\", \"bird\")\n",
        "\n",
        "    Returns:\n",
        "        X: Mel-spectrogram as float32 array with shape (128, 128, 1)\n",
        "        y: One-hot encoded label array with shape (num_classes,)\n",
        "    \"\"\"\n",
        "    # 1. Load mel-spectrogram in dB\n",
        "    S_db, sr = load_mel_spectrogram(audio_path)\n",
        "\n",
        "    # 2. Ensure fixed size 128x128\n",
        "    S_fixed = pad_or_crop_spectrogram(S_db, target_shape=(128, 128))\n",
        "\n",
        "    # 3. Normalize to [0, 1] range for better training stability\n",
        "    S_min = S_fixed.min()\n",
        "    S_max = S_fixed.max()\n",
        "    S_norm = (S_fixed - S_min) / (S_max - S_min + 1e-8)  # avoid divide-by-zero\n",
        "\n",
        "    # 4. Add channel dimension → (128, 128, 1) for CNN input\n",
        "    X = S_norm.astype(\"float32\")[..., np.newaxis]\n",
        "\n",
        "    # 5. Build one-hot label vector\n",
        "    num_classes = len(CLASS_NAMES)\n",
        "    y = np.zeros(num_classes, dtype=\"float32\")\n",
        "    y[label_to_index[label]] = 1.0\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "build a tiny dataset (even just a few examples)\n",
        "\n",
        "We’ll keep it simple: load a handful of files from each folder for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60, 128, 128, 1), (60, 3))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "For this baseline implementation, we'll load a small subset of the data to validate our pipeline. This allows us to quickly test the model architecture and training loop before scaling up.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def load_dataset(max_files_per_class: int = 20):\n",
        "    \"\"\"\n",
        "    Load audio files from all classes and convert them to model-ready format.\n",
        "\n",
        "    Args:\n",
        "        max_files_per_class: Maximum number of files to load per class\n",
        "\n",
        "    Returns:\n",
        "        X: Array of shape (N, 128, 128, 1) where N is total number of samples\n",
        "        y: Array of shape (N, num_classes) with one-hot encoded labels\n",
        "    \"\"\"\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for label in CLASS_NAMES:\n",
        "        class_dir = DATA_DIR / label\n",
        "        wav_files = sorted(class_dir.glob(\"*.wav\"))\n",
        "\n",
        "        for audio_path in wav_files[:max_files_per_class]:\n",
        "            X, y = load_example_for_model(audio_path, label)\n",
        "            X_list.append(X)\n",
        "            y_list.append(y)\n",
        "\n",
        "    # Stack individual samples into batch tensors\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    y = np.stack(y_list, axis=0)\n",
        "    return X, y\n",
        "\n",
        "# Load a small dataset for baseline testing\n",
        "X, y = load_dataset(max_files_per_class=20)\n",
        "print(f\"Dataset shape - X: {X.shape}, y: {y.shape}\")\n",
        "print(f\"Total samples: {X.shape[0]}, Classes: {y.shape[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/abhiyansainju/Desktop/GW Classes/Fall 2025/Neural Networks and Deep Learning CSCI_6366_80/Audio Classification/audio-classification-cnn/.venv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65536</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65536\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,194,368\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,213,379</span> (16.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,213,379\u001b[0m (16.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,213,379</span> (16.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,213,379\u001b[0m (16.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Dataset Structure\n",
        "\n",
        "The `load_dataset` function:\n",
        "1. Loops over each class (\"dog\", \"cat\", \"bird\")\n",
        "2. Finds all `.wav` files in each class directory\n",
        "3. Takes up to `max_files_per_class` files per class\n",
        "4. Converts each file to `(X, y)` using `load_example_for_model`\n",
        "5. Stacks all samples into batch tensors:\n",
        "   - `X.shape = (N, 128, 128, 1)` where N is the total number of samples\n",
        "   - `y.shape = (N, 3)` with one-hot encoded labels for 3 classes\n",
        "\n",
        "This gives us a small dataset to test our model pipeline before scaling up to the full dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline CNN Architecture\n",
        "\n",
        "We define a simple convolutional neural network suitable for audio classification. The architecture consists of two convolutional blocks followed by fully connected layers.\n",
        "\n",
        "### Architecture Overview\n",
        "\n",
        "**Input Shape**: `(128, 128, 1)`\n",
        "- Height: 128 mel frequency bands\n",
        "- Width: 128 time frames\n",
        "- Channels: 1 (grayscale spectrogram)\n",
        "\n",
        "**Architecture**:\n",
        "1. **Conv Block 1**: 32 filters, 3×3 kernels → MaxPooling\n",
        "2. **Conv Block 2**: 64 filters, 3×3 kernels → MaxPooling\n",
        "3. **Flatten**: Convert 2D feature maps to 1D vector\n",
        "4. **Dense Layer**: 64 neurons with ReLU activation\n",
        "5. **Output Layer**: 3 neurons (one per class) with softmax activation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "input_shape = (128, 128, 1)\n",
        "num_classes = len(CLASS_NAMES)\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Block 1: First convolutional layer\n",
        "    layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "        input_shape=input_shape,\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Block 2: Second convolutional layer\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten + Dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Layer-by-Layer Explanation\n",
        "\n",
        "**Input Layer**:\n",
        "- Shape: `(128, 128, 1)` - 128 mel bands × 128 time frames × 1 channel\n",
        "\n",
        "**Conv Block 1**:\n",
        "- **Conv2D(32 filters, 3×3)**: Learns 32 different 3×3 filters to detect local patterns\n",
        "  - Input: `(128, 128, 1)` → Output: `(128, 128, 32)` (same spatial size due to `padding=\"same\"`)\n",
        "  - ReLU activation: `ReLU(x) = max(0, x)` introduces non-linearity\n",
        "- **MaxPooling2D(2×2)**: Downsamples by taking maximum of each 2×2 block\n",
        "  - Input: `(128, 128, 32)` → Output: `(64, 64, 32)`\n",
        "\n",
        "**Conv Block 2**:\n",
        "- **Conv2D(64 filters, 3×3)**: Learns 64 more complex features\n",
        "  - Input: `(64, 64, 32)` → Output: `(64, 64, 64)`\n",
        "- **MaxPooling2D(2×2)**: Further downsampling\n",
        "  - Input: `(64, 64, 64)` → Output: `(32, 32, 64)`\n",
        "\n",
        "**Flatten**:\n",
        "- Converts `(32, 32, 64)` to 1D vector of size `32 × 32 × 64 = 65,536`\n",
        "\n",
        "**Dense(64, ReLU)**:\n",
        "- Fully connected layer with 64 neurons\n",
        "- Combines all extracted features into a compact representation\n",
        "\n",
        "**Dense(3, Softmax)**:\n",
        "- Output layer with 3 neurons (one per class: dog, cat, bird)\n",
        "- Softmax activation ensures outputs sum to 1 and are interpretable as probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sanity-check a tiny training run\n",
        "\n",
        "You don’t have to run long training now, but we can test that everything is wired correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.3125 - loss: 1.1382 - val_accuracy: 0.0000e+00 - val_loss: 1.3535\n",
            "Epoch 2/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5417 - loss: 0.9948 - val_accuracy: 0.0000e+00 - val_loss: 1.4463\n",
            "Epoch 3/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6042 - loss: 0.9258 - val_accuracy: 0.0000e+00 - val_loss: 1.8839\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compilation Parameters\n",
        "\n",
        "- **`optimizer=\"adam\"`**: Adam (Adaptive Moment Estimation) is a widely used optimizer that adapts the learning rate per parameter. It's a good default choice that often works well without extensive hyperparameter tuning.\n",
        "\n",
        "- **`loss=\"categorical_crossentropy\"`**: Suitable for multi-class classification where:\n",
        "  - We have multiple mutually exclusive classes (dog vs cat vs bird)\n",
        "  - Labels are one-hot encoded vectors: `[1, 0, 0]`, `[0, 1, 0]`, `[0, 0, 1]`\n",
        "  - Measures the difference between predicted probability distribution and true distribution\n",
        "\n",
        "- **`metrics=[\"accuracy\"]`**: Tracks the proportion of examples where the predicted class (argmax of predictions) matches the true class (argmax of true labels).\n",
        "\n",
        "The model is now ready for training!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
