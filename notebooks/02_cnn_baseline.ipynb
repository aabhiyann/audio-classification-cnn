{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Audio Classification: CNN Baseline Model\n",
        "\n",
        "**Course:** CSCI 6366 (Neural Networks and Deep Learning)  \n",
        "**Project:** Audio Classification using CNN  \n",
        "**Notebook:** Baseline CNN Model Implementation\n",
        "\n",
        "## What this notebook will do? (Plan)\n",
        "\n",
        "By the end of this new notebook, we want to have (ideally):\n",
        "\n",
        "1. A **fixed-size input representation** for each audio clip:\n",
        "   → Mel-spectrogram shaped like `(128, 128, 1)` (height × width × channels).\n",
        "\n",
        "2. A **small CNN model** defined in Keras:\n",
        "\n",
        "   ```python\n",
        "   Conv2D → ReLU → MaxPooling2D → Conv2D → MaxPooling2D → Flatten → Dense → Softmax\n",
        "   ```\n",
        "\n",
        "3. The model **compiled** with:\n",
        "   * `loss='categorical_crossentropy'`\n",
        "   * `optimizer='adam'`\n",
        "   * `metrics=['accuracy']`\n",
        "\n",
        "We'll focus on:\n",
        "* shaping the data,\n",
        "* building the model,\n",
        "* understanding every layer.\n",
        "\n",
        "(We won't worry about perfect training yet in this notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports\n",
        "\n",
        "* `numpy` → arrays and math.\n",
        "* `Path` → nice file paths.\n",
        "* `librosa` → we'll reuse it to build Mel-spectrograms.\n",
        "* `tensorflow` / `keras`:\n",
        "  * `layers` → Conv2D, MaxPooling2D, Flatten, Dense, etc.\n",
        "  * `models` → to create the `Sequential` model.\n",
        "\n",
        "If you (the person trying to run this notebook on his/her machine) get an error on TensorFlow, install it in your venv:\n",
        "\n",
        "```bash\n",
        "(.venv) pip install \"tensorflow>=2.16,<3\"\n",
        "```\n",
        "\n",
        "(run in the terminal, not in the notebook).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dog': 0, 'cat': 1, 'bird': 2}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Where our audio data lives (relative to this notebook in notebooks/)\n",
        "DATA_DIR = Path(\"../data\").resolve()\n",
        "\n",
        "# Our three classes\n",
        "CLASS_NAMES = [\"dog\", \"cat\", \"bird\"]\n",
        "\n",
        "label_to_index = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
        "label_to_index\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Basic Config and Class Labels\n",
        "\n",
        "* `DATA_DIR` points to your `data/` folder from this notebook (`../data`).\n",
        "* `CLASS_NAMES` is the list of class labels (folder names under `data/`).\n",
        "* `label_to_index` turns labels into numbers:\n",
        "  * `\"dog\" → 0`, `\"cat\" → 1`, `\"bird\" → 2`.\n",
        "* This mapping will be used for training targets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_mel_spectrogram(\n",
        "    audio_path: Path,\n",
        "    sr: int = 16000,\n",
        "    n_fft: int = 1024,\n",
        "    hop_length: int = 512,\n",
        "    n_mels: int = 128,\n",
        ") -> tuple[np.ndarray, int]:\n",
        "    \"\"\"\n",
        "    Load an audio file and compute its Mel-spectrogram in dB scale.\n",
        "\n",
        "    Returns:\n",
        "        S_db: 2D array of shape (n_mels, time_frames), Mel-spectrogram in dB.\n",
        "        sr: sample rate used.\n",
        "    \"\"\"\n",
        "    # 1. Load waveform, resampled to `sr` if needed\n",
        "    y, sr = librosa.load(audio_path, sr=sr)\n",
        "\n",
        "    # 2. Compute Mel-spectrogram (power)\n",
        "    S = librosa.feature.melspectrogram(\n",
        "        y=y,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels,\n",
        "        power=2.0,\n",
        "    )\n",
        "\n",
        "    # 3. Convert to dB scale\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "    return S_db, sr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load_mel_spectrogram Function\n",
        "\n",
        "* **Input**: path to `.wav` file.\n",
        "* **Inside**:\n",
        "  * Load waveform `y` at `sr` (16k).\n",
        "  * Compute Mel-spectrogram `(n_mels × time_frames)`.\n",
        "  * Convert to dB (log scale).\n",
        "* **Output**: `S_db` (2D spectrogram), `sr`.\n",
        "\n",
        "This function is **our bridge** from \"raw audio file\" → \"2D array we'll feed into the CNN\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad_or_crop_spectrogram(S_db: np.ndarray, target_shape=(128, 128)) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Ensure the Mel-spectrogram has shape (target_height, target_width)\n",
        "    by centrally cropping or zero-padding along the time axis.\n",
        "\n",
        "    Assumes S_db shape is (n_mels, time_frames).\n",
        "    \"\"\"\n",
        "    target_height, target_width = target_shape\n",
        "    n_mels, time_frames = S_db.shape\n",
        "\n",
        "    # 1. If mel dimension doesn't match target_height, we could pad/crop,\n",
        "    #    but here we assume n_mels == target_height (128).\n",
        "    if n_mels != target_height:\n",
        "        raise ValueError(f\"Expected {target_height} mel bands, got {n_mels}\")\n",
        "\n",
        "    # 2. If too many time frames: centrally crop to target_width\n",
        "    if time_frames > target_width:\n",
        "        start = (time_frames - target_width) // 2\n",
        "        end = start + target_width\n",
        "        S_db = S_db[:, start:end]\n",
        "\n",
        "    # 3. If too few time frames: pad with zeros on the right\n",
        "    elif time_frames < target_width:\n",
        "        pad_width = target_width - time_frames\n",
        "        S_db = np.pad(\n",
        "            S_db,\n",
        "            pad_width=((0, 0), (0, pad_width)),  # only pad time axis on the right\n",
        "            mode=\"constant\",\n",
        "            constant_values=(S_db.min(),),\n",
        "        )\n",
        "\n",
        "    # Now S_db has shape (target_height, target_width)\n",
        "    return S_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making the Spectrogram a Fixed Size (128×128)\n",
        "\n",
        "Right now different clips might have different `time_frames` (widths), depending on duration.\n",
        "\n",
        "CNNs want **fixed shape** input. So we'll:\n",
        "* Keep height = `n_mels = 128`.\n",
        "* Force width = `128` by:\n",
        "  * **If too long** → cut the center to 128 columns.\n",
        "  * **If too short** → pad with zeros on the right.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* `S_db.shape` gives `(n_mels, time_frames)`.\n",
        "* We **expect** `n_mels = 128`, and want `time_frames = 128`.\n",
        "* If `time_frames > 128`:\n",
        "  * We compute a `start` index so we crop the **center** portion.\n",
        "* If `time_frames < 128`:\n",
        "  * We pad columns on the right with the **minimum** value (darkest color).\n",
        "\n",
        "Result: every clip becomes a **128×128 matrix**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_example_for_model(audio_path: Path, label: str) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load one audio file and return:\n",
        "      X: Mel-spectrogram as float32 array with shape (128, 128, 1)\n",
        "      y: one-hot encoded label array with shape (num_classes,)\n",
        "    \"\"\"\n",
        "    # 1. Mel-spectrogram in dB\n",
        "    S_db, sr = load_mel_spectrogram(audio_path)\n",
        "\n",
        "    # 2. Ensure fixed size 128x128\n",
        "    S_fixed = pad_or_crop_spectrogram(S_db, target_shape=(128, 128))\n",
        "\n",
        "    # 3. Normalize (optional but common): scale to [0, 1]\n",
        "    #    We shift and scale based on min and max of this spectrogram\n",
        "    S_min = S_fixed.min()\n",
        "    S_max = S_fixed.max()\n",
        "    S_norm = (S_fixed - S_min) / (S_max - S_min + 1e-8)  # avoid divide-by-zero\n",
        "\n",
        "    # 4. Add channel dimension → (128, 128, 1)\n",
        "    X = S_norm.astype(\"float32\")[..., np.newaxis]\n",
        "\n",
        "    # 5. Build one-hot label vector\n",
        "    num_classes = len(CLASS_NAMES)\n",
        "    y = np.zeros(num_classes, dtype=\"float32\")\n",
        "    y[label_to_index[label]] = 1.0\n",
        "\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Audio File: Model-Ready Input & Label\n",
        "\n",
        "We make a function that:\n",
        "* Takes:\n",
        "  * `audio_path`\n",
        "  * `label` (e.g., `\"dog\"`)\n",
        "* Returns:\n",
        "  * `X`: spectrogram with shape `(128, 128, 1)` (extra channel dimension).\n",
        "  * `y`: one-hot label like `[1, 0, 0]` for dog.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "1. **Get Mel-spectrogram**:\n",
        "   ```python\n",
        "   S_db, sr = load_mel_spectrogram(audio_path)\n",
        "   ```\n",
        "\n",
        "2. **Make it 128×128**:\n",
        "   ```python\n",
        "   S_fixed = pad_or_crop_spectrogram(S_db, target_shape=(128, 128))\n",
        "   ```\n",
        "\n",
        "3. **Normalize**:\n",
        "   ```python\n",
        "   S_min = S_fixed.min()\n",
        "   S_max = S_fixed.max()\n",
        "   S_norm = (S_fixed - S_min) / (S_max - S_min + 1e-8)\n",
        "   ```\n",
        "   * This maps values to roughly `[0, 1]`.\n",
        "   * Normalization helps training.\n",
        "\n",
        "4. **Add channel dimension**:\n",
        "   ```python\n",
        "   X = S_norm.astype(\"float32\")[..., np.newaxis]\n",
        "   ```\n",
        "   * `S_norm` shape: `(128, 128)`\n",
        "   * `[..., np.newaxis]` → `(128, 128, 1)`\n",
        "     (like a grayscale image with 1 channel).\n",
        "\n",
        "5. **One-hot label**:\n",
        "   ```python\n",
        "   y = np.zeros(num_classes)\n",
        "   y[label_to_index[label]] = 1.0\n",
        "   ```\n",
        "   * For `\"dog\"` (index 0): `[1.0, 0.0, 0.0]`\n",
        "   * For `\"cat\"`: `[0.0, 1.0, 0.0]`, etc.\n",
        "\n",
        "This `(X, y)` pair is exactly what we'll feed to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((60, 128, 128, 1), (60, 3))"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_dataset(max_files_per_class: int = 20):\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for label in CLASS_NAMES:\n",
        "        class_dir = DATA_DIR / label\n",
        "        wav_files = sorted(class_dir.glob(\"*.wav\"))\n",
        "\n",
        "        for audio_path in wav_files[:max_files_per_class]:\n",
        "            X, y = load_example_for_model(audio_path, label)\n",
        "            X_list.append(X)\n",
        "            y_list.append(y)\n",
        "\n",
        "    X = np.stack(X_list, axis=0)\n",
        "    y = np.stack(y_list, axis=0)\n",
        "    return X, y\n",
        "\n",
        "X, y = load_dataset(max_files_per_class=20)\n",
        "X.shape, y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building a Tiny Dataset (Even Just a Few Examples)\n",
        "\n",
        "We'll keep it simple: load a handful of files from each folder for now.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* Loop over `\"dog\"`, `\"cat\"`, `\"bird\"`.\n",
        "* For each class:\n",
        "  * Find `.wav` files inside that folder.\n",
        "  * Take at most `max_files_per_class`.\n",
        "  * Convert each to `(X, y)` using our helper.\n",
        "* `X_list` is a Python list of arrays with shape `(128,128,1)`.\n",
        "* `np.stack` turns it into a big 4D tensor:\n",
        "  * `X.shape = (N, 128, 128, 1)`\n",
        "    (N = total number of samples).\n",
        "  * `y.shape = (N, 3)` (3 classes).\n",
        "\n",
        "This gives us a small dataset to test our model pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">65536</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,194,368</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m65536\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m4,194,368\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,213,379</span> (16.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,213,379\u001b[0m (16.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,213,379</span> (16.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,213,379\u001b[0m (16.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "input_shape = (128, 128, 1)\n",
        "num_classes = len(CLASS_NAMES)\n",
        "\n",
        "model = models.Sequential([\n",
        "    # Block 1\n",
        "    layers.Conv2D(\n",
        "        filters=32,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "        input_shape=input_shape,\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(\n",
        "        filters=64,\n",
        "        kernel_size=(3, 3),\n",
        "        activation=\"relu\",\n",
        "        padding=\"same\",\n",
        "    ),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten + Dense\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(num_classes, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the CNN Model\n",
        "\n",
        "### Detailed Explanation of Each Part\n",
        "\n",
        "#### `input_shape = (128, 128, 1)`\n",
        "\n",
        "* Height = 128 (mel bands),\n",
        "* Width = 128 (time frames),\n",
        "* Channels = 1 (grayscale spectrogram).\n",
        "\n",
        "#### `Conv2D` (first block)\n",
        "\n",
        "```python\n",
        "layers.Conv2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3, 3),\n",
        "    activation=\"relu\",\n",
        "    padding=\"same\",\n",
        "    input_shape=input_shape,\n",
        "),\n",
        "```\n",
        "\n",
        "* **filters=32**:\n",
        "  * Learn 32 different 3×3 filters.\n",
        "  * Output will have 32 feature maps.\n",
        "* **kernel_size=(3,3)**:\n",
        "  * Each filter looks at a 3×3 local patch.\n",
        "* **activation=\"relu\"**:\n",
        "  * Apply `ReLU(x) = max(0,x)` after convolution.\n",
        "* **padding=\"same\"**:\n",
        "  * Pad edges so that output height/width stay the same as input.\n",
        "* **input_shape**:\n",
        "  * Only needed in first layer:\n",
        "    * Tells Keras what input shape to expect.\n",
        "\n",
        "So after this layer:\n",
        "* Input: `(128, 128, 1)`\n",
        "* Output: `(128, 128, 32)` (same H,W, but now 32 channels).\n",
        "\n",
        "#### First `MaxPooling2D`\n",
        "\n",
        "```python\n",
        "layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "```\n",
        "\n",
        "* Pool size 2×2:\n",
        "  * Each 2×2 block in each feature map becomes 1 value.\n",
        "  * Downsamples height and width by factor 2.\n",
        "* Output:\n",
        "  * `(64, 64, 32)`.\n",
        "\n",
        "#### Second Conv block\n",
        "\n",
        "```python\n",
        "layers.Conv2D(\n",
        "    filters=64,\n",
        "    kernel_size=(3, 3),\n",
        "    activation=\"relu\",\n",
        "    padding=\"same\",\n",
        "),\n",
        "layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "```\n",
        "\n",
        "* Now we have 64 filters.\n",
        "* Input to this Conv2D: `(64, 64, 32)`.\n",
        "* Output of Conv2D: `(64, 64, 64)`.\n",
        "* After MaxPooling2D:\n",
        "  * `(32, 32, 64)`.\n",
        "\n",
        "At this stage, we have a small 32×32 spatial map with 64 channels → pretty abstract features.\n",
        "\n",
        "#### Flatten + Dense\n",
        "\n",
        "```python\n",
        "layers.Flatten(),\n",
        "layers.Dense(64, activation=\"relu\"),\n",
        "layers.Dense(num_classes, activation=\"softmax\"),\n",
        "```\n",
        "\n",
        "* **Flatten**:\n",
        "  * Take `(32, 32, 64)` and convert to 1D vector:\n",
        "    * size = `32 * 32 * 64 = 65536`.\n",
        "* **Dense(64, relu)**:\n",
        "  * Fully connected layer with 64 neurons.\n",
        "  * Combines all extracted features into a compact representation.\n",
        "* **Dense(num_classes, softmax)**:\n",
        "  * Last layer with 3 neurons (dog/cat/bird).\n",
        "  * `softmax` makes outputs sum to 1 and interpretable as probabilities.\n",
        "\n",
        "`model.summary()` prints all shapes and parameter counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compile the Model\n",
        "\n",
        "### Explanation\n",
        "\n",
        "* **optimizer=\"adam\"**:\n",
        "  * Adam is a widely used variant of gradient descent.\n",
        "  * It adapts the learning rate per parameter; good default choice.\n",
        "\n",
        "* **loss=\"categorical_crossentropy\"**:\n",
        "  * Suitable when:\n",
        "    * We have **multi-class** classification (dog vs cat vs bird).\n",
        "    * Labels are **one-hot** vectors `[1,0,0]`, `[0,1,0]`, etc.\n",
        "  * Measures how different the predicted probability distribution is from the true one.\n",
        "\n",
        "* **metrics=[\"accuracy\"]**:\n",
        "  * In training logs, show the portion of examples where:\n",
        "    * `argmax(predictions) == argmax(true_labels)`.\n",
        "\n",
        "Now the model is ready to train.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - accuracy: 0.3750 - loss: 1.1636 - val_accuracy: 0.0000e+00 - val_loss: 1.3062\n",
            "Epoch 2/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4792 - loss: 1.0155 - val_accuracy: 0.0000e+00 - val_loss: 1.4270\n",
            "Epoch 3/3\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.6042 - loss: 0.9568 - val_accuracy: 0.0000e+00 - val_loss: 1.8550\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    y,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    validation_split=0.2,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
