{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e751a9f4",
   "metadata": {},
   "source": [
    "# 05 – Transfer Learning with YAMNet Embeddings\n",
    "\n",
    "**Course:** CSCI 6366 (Neural Networks and Deep Learning)  \n",
    "**Project:** Audio Classification using CNN  \n",
    "**Notebook:** Transfer Learning with Pre-trained Audio Embeddings (YAMNet)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we extend our previous experiments (02–04) by using\n",
    "**transfer learning** with a pre-trained audio model, **YAMNet**, to classify\n",
    "animal sounds (`dog`, `cat`, `bird`).\n",
    "\n",
    "Instead of training a CNN from scratch on Mel-spectrograms, we:\n",
    "\n",
    "1. Use a pre-trained YAMNet model (trained on AudioSet) to extract\n",
    "   high-level audio **embeddings** from each waveform.\n",
    "\n",
    "2. Train a small neural network (Dense layers) **on top of these embeddings**\n",
    "   to classify our three animal classes.\n",
    "\n",
    "3. Compare this transfer-learning approach to our best CNN from\n",
    "   `04_cnn_full_data.ipynb` (CNN + Dropout 0.3).\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Reuse the same dataset and class labels as before.\n",
    "- Keep a similar **train/validation/test split** (stratified, random_state=42).\n",
    "- Evaluate test accuracy, confusion matrix, and per-class metrics.\n",
    "- Discuss how transfer learning compares to our custom CNN models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43a5a3",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "We import TensorFlow, TensorFlow Hub (for YAMNet), librosa, NumPy, and sklearn; and set constants similar to notebook 04.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c720ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths and constants\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "SAMPLE_RATE = 16000  # YAMNet expects 16 kHz audio\n",
    "\n",
    "CLASS_NAMES = [\"dog\", \"cat\", \"bird\"]\n",
    "label_to_index = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# Train/val/test ratios (match notebook 04)\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15  # of the remaining after test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afc055",
   "metadata": {},
   "source": [
    "## 2. Dataset: File Paths and Labels\n",
    "\n",
    "We reuse the same `data/` directory structure:\n",
    "\n",
    "- `data/dog/*.wav`\n",
    "- `data/cat/*.wav`\n",
    "- `data/bird/*.wav`\n",
    "\n",
    "Here we collect:\n",
    "\n",
    "- `file_paths`: list of `Path` objects to WAV files\n",
    "- `labels`: integer label indices (`0 = dog`, `1 = cat`, `2 = bird`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c42861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_file_paths_and_labels(data_dir: Path):\n",
    "    \"\"\"Collect all .wav file paths and integer labels.\"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in CLASS_NAMES:\n",
    "        class_dir = data_dir / label\n",
    "        wav_files = sorted(class_dir.glob(\"*.wav\"))\n",
    "        \n",
    "        for audio_path in wav_files:\n",
    "            file_paths.append(audio_path)\n",
    "            labels.append(label_to_index[label])\n",
    "    \n",
    "    return np.array(file_paths), np.array(labels, dtype=np.int32)\n",
    "\n",
    "file_paths, labels = collect_file_paths_and_labels(DATA_DIR)\n",
    "print(\"Total files:\", len(file_paths))\n",
    "for idx, label_name in enumerate(CLASS_NAMES):\n",
    "    count = np.sum(labels == idx)\n",
    "    print(f\"{label_name}: {count} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd4fae",
   "metadata": {},
   "source": [
    "## 3. Stratified Train / Validation / Test Split\n",
    "\n",
    "We create explicit train, validation, and test sets using stratified splits.\n",
    "We use the **same ratios and random_state=42** as in `04_cnn_full_data.ipynb` so\n",
    "the splits are comparable:\n",
    "\n",
    "- Test set: 15% of data\n",
    "- From the remaining 85%, we take 15% as validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3334bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split off test set\n",
    "paths_train_full, paths_test, y_train_full, y_test = train_test_split(\n",
    "    file_paths,\n",
    "    labels,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "# Now split train_full into train and validation\n",
    "paths_train, paths_val, y_train, y_val = train_test_split(\n",
    "    paths_train_full,\n",
    "    y_train_full,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(paths_train))\n",
    "print(\"Validation size:\", len(paths_val))\n",
    "print(\"Test size:\", len(paths_test))\n",
    "\n",
    "def print_split_stats(name, y_split):\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for idx, label_name in enumerate(CLASS_NAMES):\n",
    "        count = np.sum(y_split == idx)\n",
    "        print(f\"  {label_name}: {count}\")\n",
    "\n",
    "print_split_stats(\"Train\", y_train)\n",
    "print_split_stats(\"Validation\", y_val)\n",
    "print_split_stats(\"Test\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db4af7",
   "metadata": {},
   "source": [
    "## 4. YAMNet: Pre-trained Audio Model\n",
    "\n",
    "We use **YAMNet**, a convolutional neural network trained on Google's\n",
    "AudioSet dataset. It takes a 16 kHz mono waveform and outputs:\n",
    "\n",
    "- Class scores (for many audio event classes),\n",
    "- Intermediate **embeddings** (1024-dimensional vectors),\n",
    "- Log Mel-spectrogram (internal representation).\n",
    "\n",
    "For transfer learning, we will:\n",
    "\n",
    "1. Load each audio file at 16 kHz (mono).\n",
    "2. Pass the waveform through YAMNet.\n",
    "3. Take the **average** of all frame-level embeddings to get a single\n",
    "   1024-D embedding vector per clip.\n",
    "4. Use these embeddings as input features to a small classifier network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad957f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YAMNet TF Hub handle (internet needed the first time you run this)\n",
    "YAMNET_HANDLE = \"https://tfhub.dev/google/yamnet/1\"\n",
    "\n",
    "print(\"Loading YAMNet from TensorFlow Hub...\")\n",
    "yamnet_model = hub.load(YAMNET_HANDLE)\n",
    "print(\"YAMNet loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add8661",
   "metadata": {},
   "source": [
    "## 5. Helper Functions: Waveforms → YAMNet Embeddings\n",
    "\n",
    "We now define:\n",
    "\n",
    "- `load_waveform(path)`: load a mono waveform at 16 kHz using `librosa`.\n",
    "- `yamnet_embedding_for_file(path)`: run YAMNet and average frame-level\n",
    "  embeddings to get a single 1024-D vector.\n",
    "- `compute_embeddings(file_paths)`: apply this to a list of paths and\n",
    "  return an array of shape `(N, embedding_dim)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(path: Path, sample_rate: int = SAMPLE_RATE) -> np.ndarray:\n",
    "    \"\"\"Load a mono waveform at the desired sample rate.\"\"\"\n",
    "    y, sr = librosa.load(path, sr=sample_rate, mono=True)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def yamnet_embedding_for_file(path: Path) -> np.ndarray:\n",
    "    \"\"\"Compute a single YAMNet embedding vector for one audio file.\"\"\"\n",
    "    waveform = load_waveform(path)  # shape: (num_samples,)\n",
    "    \n",
    "    # YAMNet expects a batch dimension: (N, num_samples)\n",
    "    waveform_tf = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "    waveform_tf = tf.reshape(waveform_tf, [1, -1])\n",
    "    \n",
    "    # YAMNet returns (scores, embeddings, spectrogram)\n",
    "    scores, embeddings, spectrogram = yamnet_model(waveform_tf)\n",
    "    # embeddings shape: (num_frames, 1024)\n",
    "    \n",
    "    # Average over time frames to get a single 1024-D vector\n",
    "    embedding_mean = tf.reduce_mean(embeddings, axis=0)  # shape: (1024,)\n",
    "    return embedding_mean.numpy()\n",
    "\n",
    "def compute_embeddings(file_paths: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute YAMNet embeddings for a list/array of file paths.\"\"\"\n",
    "    all_embeddings = []\n",
    "    for i, path in enumerate(file_paths):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing file {i}/{len(file_paths)}: {path.name}\")\n",
    "        emb = yamnet_embedding_for_file(path)\n",
    "        all_embeddings.append(emb)\n",
    "    return np.stack(all_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f8b36",
   "metadata": {},
   "source": [
    "## 6. Build Embeddings for Train / Validation / Test\n",
    "\n",
    "We now compute YAMNet embeddings for:\n",
    "\n",
    "- `paths_train` → `X_train_embed`\n",
    "- `paths_val` → `X_val_embed`\n",
    "- `paths_test` → `X_test_embed`\n",
    "\n",
    "Then we convert integer labels (`0,1,2`) into one-hot vectors of length 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings (this may take some time on CPU)\n",
    "X_train_embed = compute_embeddings(paths_train)\n",
    "X_val_embed = compute_embeddings(paths_val)\n",
    "X_test_embed = compute_embeddings(paths_test)\n",
    "\n",
    "print(\"Embedding shapes:\")\n",
    "print(\"  Train:\", X_train_embed.shape)\n",
    "print(\"  Val:  \", X_val_embed.shape)\n",
    "print(\"  Test: \", X_test_embed.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "def to_one_hot(y_int: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes), dtype=np.float32)\n",
    "    for i, idx in enumerate(y_int):\n",
    "        y_one_hot[i, idx] = 1.0\n",
    "    return y_one_hot\n",
    "\n",
    "y_train_oh = to_one_hot(y_train, num_classes)\n",
    "y_val_oh = to_one_hot(y_val, num_classes)\n",
    "y_test_oh = to_one_hot(y_test, num_classes)\n",
    "\n",
    "print(\"Label shapes (one-hot):\")\n",
    "print(\"  Train:\", y_train_oh.shape)\n",
    "print(\"  Val:  \", y_val_oh.shape)\n",
    "print(\"  Test: \", y_test_oh.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d5b6a",
   "metadata": {},
   "source": [
    "## 7. Plotting Helper for Training Curves\n",
    "\n",
    "We reuse the same helper function to plot training and validation loss\n",
    "and accuracy over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, title_prefix=\"\"):\n",
    "    \"\"\"Plot training and validation loss/accuracy.\"\"\"\n",
    "    history_dict = history.history\n",
    "\n",
    "    train_loss = history_dict.get(\"loss\", [])\n",
    "    val_loss = history_dict.get(\"val_loss\", [])\n",
    "    train_acc = history_dict.get(\"accuracy\", [])\n",
    "    val_acc = history_dict.get(\"val_accuracy\", [])\n",
    "\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label=\"Train loss\")\n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label=\"Train acc\")\n",
    "    if val_acc:\n",
    "        plt.plot(epochs, val_acc, label=\"Val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6208e",
   "metadata": {},
   "source": [
    "## 8. Define the Transfer-Learning Classifier\n",
    "\n",
    "We now define a simple classifier that takes YAMNet embeddings as input:\n",
    "\n",
    "- Input: 1024-D embedding vector\n",
    "- Dense(128, ReLU) + Dropout(0.3)\n",
    "- Dense(3, Softmax)\n",
    "\n",
    "We use the same loss and optimizer as before (`categorical_crossentropy` +\n",
    "`adam`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yamnet_classifier(input_dim: int, num_classes: int = 3, dropout_rate: float = 0.3):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_dim = X_train_embed.shape[1]\n",
    "yamnet_model_head = build_yamnet_classifier(input_dim=input_dim, num_classes=num_classes)\n",
    "yamnet_model_head.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf6781",
   "metadata": {},
   "source": [
    "## 9. Training the YAMNet Embedding Classifier\n",
    "\n",
    "We train the classifier on top of YAMNet embeddings using the same\n",
    "train/validation split as before.\n",
    "\n",
    "- Epochs: 20\n",
    "- Batch size: 16\n",
    "\n",
    "(You can adjust these if training is very fast or slow.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab46f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "yamnet_history = yamnet_model_head.fit(\n",
    "    X_train_embed,\n",
    "    y_train_oh,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val_embed, y_val_oh),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "plot_training_curves(yamnet_history, title_prefix=\"YAMNet Embedding Classifier\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
