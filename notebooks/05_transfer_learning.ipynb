{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e751a9f4",
   "metadata": {},
   "source": [
    "# 05 – Transfer Learning with YAMNet Embeddings\n",
    "\n",
    "**Course:** CSCI 6366 (Neural Networks and Deep Learning)  \n",
    "**Project:** Audio Classification using CNN  \n",
    "**Notebook:** Transfer Learning with Pre-trained Audio Embeddings (YAMNet)\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we extend our previous experiments (02–04) by using\n",
    "**transfer learning** with a pre-trained audio model, **YAMNet**, to classify\n",
    "animal sounds (`dog`, `cat`, `bird`).\n",
    "\n",
    "Instead of training a CNN from scratch on Mel-spectrograms, we:\n",
    "\n",
    "1. Use a pre-trained YAMNet model (trained on AudioSet) to extract\n",
    "   high-level audio **embeddings** from each waveform.\n",
    "\n",
    "2. Train a small neural network (Dense layers) **on top of these embeddings**\n",
    "   to classify our three animal classes.\n",
    "\n",
    "3. Compare this transfer-learning approach to our best CNN from\n",
    "   `04_cnn_full_data.ipynb` (CNN + Dropout 0.3).\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Reuse the same dataset and class labels as before.\n",
    "- Keep a similar **train/validation/test split** (stratified, random_state=42).\n",
    "- Evaluate test accuracy, confusion matrix, and per-class metrics.\n",
    "- Discuss how transfer learning compares to our custom CNN models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43a5a3",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "We import TensorFlow, TensorFlow Hub (for YAMNet), librosa, NumPy, and sklearn; and set constants similar to notebook 04.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c720ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths and constants\n",
    "DATA_DIR = Path(\"../data\").resolve()\n",
    "SAMPLE_RATE = 16000  # YAMNet expects 16 kHz audio\n",
    "\n",
    "CLASS_NAMES = [\"dog\", \"cat\", \"bird\"]\n",
    "label_to_index = {label: idx for idx, label in enumerate(CLASS_NAMES)}\n",
    "\n",
    "# Train/val/test ratios (match notebook 04)\n",
    "TEST_SIZE = 0.15\n",
    "VAL_SIZE = 0.15  # of the remaining after test split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95afc055",
   "metadata": {},
   "source": [
    "## 2. Dataset: File Paths and Labels\n",
    "\n",
    "We reuse the same `data/` directory structure:\n",
    "\n",
    "- `data/dog/*.wav`\n",
    "- `data/cat/*.wav`\n",
    "- `data/bird/*.wav`\n",
    "\n",
    "Here we collect:\n",
    "\n",
    "- `file_paths`: list of `Path` objects to WAV files\n",
    "- `labels`: integer label indices (`0 = dog`, `1 = cat`, `2 = bird`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c42861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 610\n",
      "dog: 210 files\n",
      "cat: 207 files\n",
      "bird: 193 files\n"
     ]
    }
   ],
   "source": [
    "def collect_file_paths_and_labels(data_dir: Path):\n",
    "    \"\"\"Collect all .wav file paths and integer labels.\"\"\"\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in CLASS_NAMES:\n",
    "        class_dir = data_dir / label\n",
    "        wav_files = sorted(class_dir.glob(\"*.wav\"))\n",
    "        \n",
    "        for audio_path in wav_files:\n",
    "            file_paths.append(audio_path)\n",
    "            labels.append(label_to_index[label])\n",
    "    \n",
    "    return np.array(file_paths), np.array(labels, dtype=np.int32)\n",
    "\n",
    "file_paths, labels = collect_file_paths_and_labels(DATA_DIR)\n",
    "print(\"Total files:\", len(file_paths))\n",
    "for idx, label_name in enumerate(CLASS_NAMES):\n",
    "    count = np.sum(labels == idx)\n",
    "    print(f\"{label_name}: {count} files\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd4fae",
   "metadata": {},
   "source": [
    "## 3. Stratified Train / Validation / Test Split\n",
    "\n",
    "We create explicit train, validation, and test sets using stratified splits.\n",
    "We use the **same ratios and random_state=42** as in `04_cnn_full_data.ipynb` so\n",
    "the splits are comparable:\n",
    "\n",
    "- Test set: 15% of data\n",
    "- From the remaining 85%, we take 15% as validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3334bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 440\n",
      "Validation size: 78\n",
      "Test size: 92\n",
      "\n",
      "Train distribution:\n",
      "  dog: 151\n",
      "  cat: 150\n",
      "  bird: 139\n",
      "\n",
      "Validation distribution:\n",
      "  dog: 27\n",
      "  cat: 26\n",
      "  bird: 25\n",
      "\n",
      "Test distribution:\n",
      "  dog: 32\n",
      "  cat: 31\n",
      "  bird: 29\n"
     ]
    }
   ],
   "source": [
    "# First split off test set\n",
    "paths_train_full, paths_test, y_train_full, y_test = train_test_split(\n",
    "    file_paths,\n",
    "    labels,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=labels,\n",
    ")\n",
    "\n",
    "# Now split train_full into train and validation\n",
    "paths_train, paths_val, y_train, y_val = train_test_split(\n",
    "    paths_train_full,\n",
    "    y_train_full,\n",
    "    test_size=VAL_SIZE,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(paths_train))\n",
    "print(\"Validation size:\", len(paths_val))\n",
    "print(\"Test size:\", len(paths_test))\n",
    "\n",
    "def print_split_stats(name, y_split):\n",
    "    print(f\"\\n{name} distribution:\")\n",
    "    for idx, label_name in enumerate(CLASS_NAMES):\n",
    "        count = np.sum(y_split == idx)\n",
    "        print(f\"  {label_name}: {count}\")\n",
    "\n",
    "print_split_stats(\"Train\", y_train)\n",
    "print_split_stats(\"Validation\", y_val)\n",
    "print_split_stats(\"Test\", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3db4af7",
   "metadata": {},
   "source": [
    "## 4. YAMNet: Pre-trained Audio Model\n",
    "\n",
    "We use **YAMNet**, a convolutional neural network trained on Google's\n",
    "AudioSet dataset. It takes a 16 kHz mono waveform and outputs:\n",
    "\n",
    "- Class scores (for many audio event classes),\n",
    "- Intermediate **embeddings** (1024-dimensional vectors),\n",
    "- Log Mel-spectrogram (internal representation).\n",
    "\n",
    "For transfer learning, we will:\n",
    "\n",
    "1. Load each audio file at 16 kHz (mono).\n",
    "2. Pass the waveform through YAMNet.\n",
    "3. Take the **average** of all frame-level embeddings to get a single\n",
    "   1024-D embedding vector per clip.\n",
    "4. Use these embeddings as input features to a small classifier network.\n",
    "\n",
    "**Note on YAMNet loading:**\n",
    "\n",
    "On some macOS Python installations, HTTPS certificate issues can prevent\n",
    "`tensorflow_hub` from downloading models from `tfhub.dev`. To make our project\n",
    "reproducible and avoid SSL problems, we manually downloaded the YAMNet SavedModel\n",
    "and load it from a local path (`../models/yamnet/`) instead of from the remote URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad957f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local YAMNet model at: /Users/abhiyansainju/Desktop/GW Classes/Fall 2025/Neural Networks and Deep Learning CSCI_6366_80/Audio Classification/audio-classification-cnn/models/yamnet\n",
      "YAMNet loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load YAMNet from local path (downloaded SavedModel)\n",
    "# This avoids SSL certificate issues and makes the project reproducible\n",
    "YAMNET_HANDLE = Path(\"../models/yamnet\").resolve().as_posix()\n",
    "\n",
    "print(\"Using local YAMNet model at:\", YAMNET_HANDLE)\n",
    "yamnet_model = hub.load(YAMNET_HANDLE)\n",
    "print(\"YAMNet loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add8661",
   "metadata": {},
   "source": [
    "## 5. Helper Functions: Waveforms → YAMNet Embeddings\n",
    "\n",
    "We now define:\n",
    "\n",
    "- `load_waveform(path)`: load a mono waveform at 16 kHz using `librosa`.\n",
    "- `yamnet_embedding_for_file(path)`: run YAMNet and average frame-level\n",
    "  embeddings to get a single 1024-D vector.\n",
    "- `compute_embeddings(file_paths)`: apply this to a list of paths and\n",
    "  return an array of shape `(N, embedding_dim)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform(path: Path, sample_rate: int = SAMPLE_RATE) -> np.ndarray:\n",
    "    \"\"\"Load a mono waveform at the desired sample rate.\"\"\"\n",
    "    y, sr = librosa.load(path, sr=sample_rate, mono=True)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def yamnet_embedding_for_file(path: Path) -> np.ndarray:\n",
    "    \"\"\"Compute YAMNet embedding (1024-D) for a single audio file.\"\"\"\n",
    "    # 1. Load audio at 16 kHz (mono)\n",
    "    waveform, sr = librosa.load(path, sr=16000, mono=True)\n",
    "    waveform = waveform.astype(np.float32)\n",
    "    \n",
    "    # 2. Convert to Tensor *without* adding a batch dimension\n",
    "    waveform_tf = tf.convert_to_tensor(waveform, dtype=tf.float32)\n",
    "    # IMPORTANT: do NOT reshape to (1, -1). YAMNet expects shape (num_samples,)\n",
    "    \n",
    "    # 3. Run YAMNet\n",
    "    # YAMNet returns: scores, embeddings, spectrogram\n",
    "    scores, embeddings, spectrogram = yamnet_model(waveform_tf)\n",
    "    \n",
    "    # 4. Average embeddings over time frames to get a single 1024-D vector\n",
    "    embedding_mean = tf.reduce_mean(embeddings, axis=0)  # shape: (1024,)\n",
    "    \n",
    "    # 5. Return as NumPy array\n",
    "    return embedding_mean.numpy()\n",
    "\n",
    "def compute_embeddings(file_paths: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute YAMNet embeddings for a list/array of file paths.\"\"\"\n",
    "    all_embeddings = []\n",
    "    for i, path in enumerate(file_paths):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing file {i}/{len(file_paths)}: {path.name}\")\n",
    "        emb = yamnet_embedding_for_file(path)\n",
    "        all_embeddings.append(emb)\n",
    "    return np.stack(all_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f8b36",
   "metadata": {},
   "source": [
    "## 6. Build Embeddings for Train / Validation / Test\n",
    "\n",
    "We now compute YAMNet embeddings for:\n",
    "\n",
    "- `paths_train` → `X_train_embed`\n",
    "- `paths_val` → `X_val_embed`\n",
    "- `paths_test` → `X_test_embed`\n",
    "\n",
    "Then we convert integer labels (`0,1,2`) into one-hot vectors of length 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9179fa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 0/440: 5236848b_nohash_0.wav\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(1, 14336), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor: shape=(1, 14336), dtype=float32, numpy=\narray([[ 0.00100708,  0.00045776,  0.00021362, ..., -0.00170898,\n        -0.00097656, -0.00152588]], shape=(1, 14336), dtype=float32)>,) and kwargs: {} for signature: (waveform: TensorSpec(shape=(None,), dtype=tf.float32, name=None)).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Compute embeddings (this may take some time on CPU)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train_embed = \u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m X_val_embed = compute_embeddings(paths_val)\n\u001b[32m      4\u001b[39m X_test_embed = compute_embeddings(paths_test)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mcompute_embeddings\u001b[39m\u001b[34m(file_paths)\u001b[39m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m50\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(file_paths)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     emb = \u001b[43myamnet_embedding_for_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m     all_embeddings.append(emb)\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.stack(all_embeddings, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36myamnet_embedding_for_file\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     12\u001b[39m waveform_tf = tf.reshape(waveform_tf, [\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m])\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# YAMNet returns (scores, embeddings, spectrogram)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m scores, embeddings, spectrogram = \u001b[43myamnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform_tf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# embeddings shape: (num_frames, 1024)\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Average over time frames to get a single 1024-D vector\u001b[39;00m\n\u001b[32m     19\u001b[39m embedding_mean = tf.reduce_mean(embeddings, axis=\u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# shape: (1024,)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GW Classes/Fall 2025/Neural Networks and Deep Learning CSCI_6366_80/Audio Classification/audio-classification-cnn/.venv/lib/python3.11/site-packages/tensorflow/python/saved_model/load.py:817\u001b[39m, in \u001b[36m_call_attribute\u001b[39m\u001b[34m(instance, *args, **kwargs)\u001b[39m\n\u001b[32m    816\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_attribute\u001b[39m(instance, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m817\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GW Classes/Fall 2025/Neural Networks and Deep Learning CSCI_6366_80/Audio Classification/audio-classification-cnn/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/GW Classes/Fall 2025/Neural Networks and Deep Learning CSCI_6366_80/Audio Classification/audio-classification-cnn/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py:446\u001b[39m, in \u001b[36mbind_function_inputs\u001b[39m\u001b[34m(args, kwargs, function_type, default_values)\u001b[39m\n\u001b[32m    442\u001b[39m   bound_arguments = function_type.bind_with_defaults(\n\u001b[32m    443\u001b[39m       args, sanitized_kwargs, default_values\n\u001b[32m    444\u001b[39m   )\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    447\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBinding inputs to tf.function failed due to `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    448\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and kwargs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msanitized_kwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for signature:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m   ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m bound_arguments\n",
      "\u001b[31mTypeError\u001b[39m: Binding inputs to tf.function failed due to `Can not cast TensorSpec(shape=(1, 14336), dtype=tf.float32, name=None) to TensorSpec(shape=(None,), dtype=tf.float32, name=None)`. Received args: (<tf.Tensor: shape=(1, 14336), dtype=float32, numpy=\narray([[ 0.00100708,  0.00045776,  0.00021362, ..., -0.00170898,\n        -0.00097656, -0.00152588]], shape=(1, 14336), dtype=float32)>,) and kwargs: {} for signature: (waveform: TensorSpec(shape=(None,), dtype=tf.float32, name=None))."
     ]
    }
   ],
   "source": [
    "# Compute embeddings (this may take some time on CPU)\n",
    "X_train_embed = compute_embeddings(paths_train)\n",
    "X_val_embed = compute_embeddings(paths_val)\n",
    "X_test_embed = compute_embeddings(paths_test)\n",
    "\n",
    "print(\"Embedding shapes:\")\n",
    "print(\"  Train:\", X_train_embed.shape)\n",
    "print(\"  Val:  \", X_val_embed.shape)\n",
    "print(\"  Test: \", X_test_embed.shape)\n",
    "\n",
    "# One-hot encode labels\n",
    "num_classes = len(CLASS_NAMES)\n",
    "\n",
    "def to_one_hot(y_int: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    y_one_hot = np.zeros((len(y_int), num_classes), dtype=np.float32)\n",
    "    for i, idx in enumerate(y_int):\n",
    "        y_one_hot[i, idx] = 1.0\n",
    "    return y_one_hot\n",
    "\n",
    "y_train_oh = to_one_hot(y_train, num_classes)\n",
    "y_val_oh = to_one_hot(y_val, num_classes)\n",
    "y_test_oh = to_one_hot(y_test, num_classes)\n",
    "\n",
    "print(\"Label shapes (one-hot):\")\n",
    "print(\"  Train:\", y_train_oh.shape)\n",
    "print(\"  Val:  \", y_val_oh.shape)\n",
    "print(\"  Test: \", y_test_oh.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d5b6a",
   "metadata": {},
   "source": [
    "## 7. Plotting Helper for Training Curves\n",
    "\n",
    "We reuse the same helper function to plot training and validation loss\n",
    "and accuracy over epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee93a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(history, title_prefix=\"\"):\n",
    "    \"\"\"Plot training and validation loss/accuracy.\"\"\"\n",
    "    history_dict = history.history\n",
    "\n",
    "    train_loss = history_dict.get(\"loss\", [])\n",
    "    val_loss = history_dict.get(\"val_loss\", [])\n",
    "    train_acc = history_dict.get(\"accuracy\", [])\n",
    "    val_acc = history_dict.get(\"val_accuracy\", [])\n",
    "\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_loss, label=\"Train loss\")\n",
    "    if val_loss:\n",
    "        plt.plot(epochs, val_loss, label=\"Val loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{title_prefix} Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_acc, label=\"Train acc\")\n",
    "    if val_acc:\n",
    "        plt.plot(epochs, val_acc, label=\"Val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{title_prefix} Training vs Validation Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6208e",
   "metadata": {},
   "source": [
    "## 8. Define the Transfer-Learning Classifier\n",
    "\n",
    "We now define a simple classifier that takes YAMNet embeddings as input:\n",
    "\n",
    "- Input: 1024-D embedding vector\n",
    "- Dense(128, ReLU) + Dropout(0.3)\n",
    "- Dense(3, Softmax)\n",
    "\n",
    "We use the same loss and optimizer as before (`categorical_crossentropy` +\n",
    "`adam`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_yamnet_classifier(input_dim: int, num_classes: int = 3, dropout_rate: float = 0.3):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "input_dim = X_train_embed.shape[1]\n",
    "yamnet_model_head = build_yamnet_classifier(input_dim=input_dim, num_classes=num_classes)\n",
    "yamnet_model_head.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbf6781",
   "metadata": {},
   "source": [
    "## 9. Training the YAMNet Embedding Classifier\n",
    "\n",
    "We train the classifier on top of YAMNet embeddings using the same\n",
    "train/validation split as before.\n",
    "\n",
    "- Epochs: 20\n",
    "- Batch size: 16\n",
    "\n",
    "(You can adjust these if training is very fast or slow.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab46f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "yamnet_history = yamnet_model_head.fit(\n",
    "    X_train_embed,\n",
    "    y_train_oh,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val_embed, y_val_oh),\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "plot_training_curves(yamnet_history, title_prefix=\"YAMNet Embedding Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f99dce9",
   "metadata": {},
   "source": [
    "## 10. Evaluation on the Held-out Test Set\n",
    "\n",
    "We now evaluate the YAMNet-based classifier on the test set (92 clips),\n",
    "and compute:\n",
    "\n",
    "- Test loss and accuracy\n",
    "- Confusion matrix\n",
    "- Per-class precision, recall, and F1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252598a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_embeddings(model, X_test, y_test_oh, y_test_int, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate a classifier on embedding features and print metrics.\"\"\"\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_oh, verbose=0)\n",
    "    print(f\"{model_name} - Test loss: {test_loss:.4f}, Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    y_pred_probs = model.predict(X_test, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    cm = confusion_matrix(y_test_int, y_pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test_int, y_pred, target_names=CLASS_NAMES))\n",
    "\n",
    "    return test_loss, test_acc, cm\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"YAMNet Embedding Classifier RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "yamnet_results = evaluate_model_embeddings(\n",
    "    yamnet_model_head,\n",
    "    X_test_embed,\n",
    "    y_test_oh,\n",
    "    y_test,\n",
    "    model_name=\"YAMNet + Dense Head\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
